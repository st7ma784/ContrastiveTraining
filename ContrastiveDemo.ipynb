{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "from functools import reduce\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def oneminus(args):\n",
    "    return 1-args\n",
    "\n",
    "\n",
    "def calculate_loss(*Args):\n",
    "    \n",
    "    return torch.sqrt(torch.abs(torch.sum(reduce(torch.add,[torch.pow(arg,2).view(*list([1]*i+[arg.shape[0]]+[1]*(len(Args)-1-i)+[-1])) for i,arg in enumerate(Args)]).sub_(\n",
    "                            torch.pow(reduce(torch.add,[arg.view(*list([1]*i+[arg.shape[0]]+[1]*(len(Args)-1-i)+[-1])) for i,arg in enumerate(Args)]),2),alpha=1/len(Args)),dim=-1)))\n",
    "\n",
    "def lossfn(*args):\n",
    "    return oneminus(calculate_loss(*args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PTLModule(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                batch_size=16,\n",
    "                learning_rate=0.00001,\n",
    "                n=6,):\n",
    "        super().__init__(\n",
    "           \n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "        self.emb=nn.Embedding(1000, 32)\n",
    "        self.layer1 = nn.Linear(32, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.batch_size=batch_size\n",
    "        self.learning_rate=learning_rate\n",
    "        self.calculate_loss=lossfn\n",
    "        self.n=n\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        self.loss=nn.CrossEntropyLoss()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    def setup(self, stage):\n",
    "        self.train_dataset = torch.utils.data.TensorDataset(torch.randint(0, 1000, (10000,)))\n",
    "\n",
    "    def train_dataloader(self,batch_size=32):\n",
    "      \n",
    "        import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "\n",
    "        return dataloader.DataLoader(self.train_dataset,batch_size=self.batch_size,shuffle=True,num_workers=8,drop_last=True)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x=self.emb(batch[0]) # should be Bxf \n",
    "        nx=[self(x+torch.randn_like(x))]*self.n # should be Bxf\n",
    "\n",
    "        logits=torch.mul(torch.nan_to_num(self.calculate_loss(*nx)),self.logit_scale.exp())\n",
    "        labels=torch.ones_like(batch[0],dtype=torch.float)\n",
    "        while len(labels.shape)<len(logits.shape):\n",
    "            labels=torch.diag_embed(labels)\n",
    "        #labels=torch.nan_to_num(labels)\n",
    "        \n",
    "        loss = self.loss(logits, labels)\n",
    "        self.log(\"loss\",loss,enable_graph=False)\n",
    "        return  {\"loss\":loss, \"labels\":batch[0], \"embs\":nx[0]}  \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "   \n",
    "        alllabels=torch.cat([x['labels'] for x in outputs],dim=0)\n",
    "        allembs=torch.cat([x['embs'] for x in outputs],dim=0)\n",
    "       \n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "        reg =LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=0, n_jobs=-1)\n",
    "        reg.fit(allembs.detach().cpu().numpy(), alllabels.detach().cpu().numpy())\n",
    "        \n",
    "        self.log(\"score\",reg.score(allembs.detach().cpu().numpy(), alllabels.detach().cpu().numpy()),on_step=False,on_epoch=True,prog_bar=True,logger=True)\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [p for p in self.parameters()], lr=self.hparams.learning_rate, eps=10e-8,            )\n",
    "        return [optimizer]\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "class PTLModuleStock(PTLModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "     \n",
    "        x=self.emb(batch[0]) # should be Bxf \n",
    "        nx=[self(x+torch.randn_like(x))]*self.n # should be Bxf\n",
    "        loss=reduce(torch.add,[self.loss(item@ x.T *self.logit_scale.exp(),torch.arange(batch[0].shape[0],device=self.device),alpha=self.alpha) for x in nx  for item in nx])\n",
    "     \n",
    "        self.log(\"loss\",loss,enable_graph=False)\n",
    "        return  {\"loss\":loss, \"labels\":batch[0], \"embs\":nx[0]}  \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we're going to create some cool graphs, each with epochs : score for each of the 6 models and for each method. \n",
    "results={n:{ i:{} for i in range(17)} for n in range(2,14)}\n",
    "\n",
    "for n in range(2,14):\n",
    "    i=5\n",
    "    model=PTLModule(logitsversion=i)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        gpus=1,\n",
    "        max_epochs=20,\n",
    "        logger=TensorBoardLogger(\"tb_logs\"),\n",
    "        auto_scale_batch_size=\"binsearch\",\n",
    "        auto_lr_find=False,\n",
    "    )\n",
    "    trainer.tune(model)\n",
    "\n",
    "    trainer.fit(model)\n",
    "    results[n][i]=model.trainer.logged_metrics\n",
    "\n",
    "\n",
    "for n in range(2,14):\n",
    "    #do benchmark first\n",
    "    model=PTLModuleStock(n=n)\n",
    "    trainer = Trainer(\n",
    "        gpus=1,\n",
    "        max_epochs=20,\n",
    "        logger=TensorBoardLogger(\"tb_logs\"),\n",
    "        auto_scale_batch_size=\"binsearch\",\n",
    "        auto_lr_find=False)\n",
    "    try:\n",
    "        trainer.tune(model)\n",
    "\n",
    "        trainer.fit(model)\n",
    "        results[n][\"stock\"]=model.trainer.logged_metrics\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        results[n][\"stock\"]=None\n",
    "\n",
    "#save results\n",
    "import pickle\n",
    "with open(\"base-v-.pkl\",\"wb\") as f:\n",
    "    pickle.dump(results,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"base-v-.pkl\",\"rb\") as f:\n",
    "    base=pickle.load(f)\n",
    "#pickle.load(\"base-v-5.pkl\")\n",
    "stockscores=[base[n][\"stock\"][\"score\"].item()  for n in base]\n",
    "scores=[base[n][5][\"score\"].item()  for n in base]\n",
    "\n",
    "#plot n against score for both \n",
    "\n",
    "plt.plot(np.arange(2,14),scores,label=\"v5\")\n",
    "plt.plot(np.arange(2,14),stockscores,label=\"stock\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title(\"Score for different n\")\n",
    "plt.savefig(\"scorev5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
